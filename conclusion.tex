\section{Conclusion}
A perennial challenge in data analytics is presence of dirty data
in the form of missing, duplicate, incorrect or inconsistent values.
Data analysts report that data cleaning remains one of the most time
consuming steps in the analysis process, and data cleaning can require
a significant amount of developer effort in writing software or rules
to fix the corruption. 

To the best of our knowledge, this is the first work to marry data cleaning with sampling-based query processing.
In traditional AQP, approximation necessarily sacrifices accuracy for reduced latency. 
However, SampleClean tradesoff data transformation cost for gradual improvements in query accuracy.
While SampleClean introduces approximation error, the data cleaning mitigates errors in query results.
The result is bounded approximations of clean aggregate query results.
Since \emph{a priori}, the magnitude of data
error is unknown, any amount of error is potentially biasing, and
without SampleClean, analysts face a dichotomy between facing the cost of data cleaning
or coping with consequences of unknown inaccuracy.

\vspace{1.5em}

\textbf{\small This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, C3Energy, Cisco, Cray, Cloudera, EMC2, Ericsson, Facebook, Guavus, HP, Huawei, Informatica, Intel, Microsoft, NetApp, Pivotal, Samsung, Schlumberger, Splunk, Virdata and VMware.}