\section{Introduction}
Data are susceptible to various forms of corruption such as missing,
incorrect, or inconsistent representations \cite{Gartner}.
Real-world data are commonly integrated from multiple sources, and the integration process may lead to a variety of errors~\cite{DBLP:journals/pvldb/DongS13}. 
Data analysts report that data cleaning remains one of the most time
consuming steps in the analysis process \cite{nytimes}.
Identifying and fixing data error often requires manually inspecting data, which can quickly become costly and time-consuming. 
While crowdsourcing is an increasingly viable option for correcting some types of errors ~\cite{DBLP:conf/sigmod/JefferyFH08,DBLP:journals/pvldb/FanLMTY10,DBLP:journals/pvldb/YakoutENOI11, gokhale2014corleone, park2014crowdfill, sampleclean,chu2015katara}, it comes at the significant cost of additional latency and the overhead of managing human workers. 

On the other hand, ignoring the effects of dirty data is potentially dangerous.
Analysts have to choose between facing the cost of data cleaning
or coping with consequences of unknown inaccuracy.
In this article, we describe a middle ground that we call SampleClean.
SampleClean leverages insights from statistical estimation theory, approximate query processing, and data cleaning to devise algorithms for estimating query results when
only a sample of data is cleaned.
The intriguing part of SampleClean is that results computed using a small clean sample,                                   
results can be more accurate than those computed over the full data due to the data cleaning.
This approximation error is boundable, unlike the unknown data error, and the tightness of the bound is parametrized by a flexible cleaning cost (i.e., the sampling size).

The motivation behind SampleClean is in many ways analogous to that of Approximate Query Processing (AQP) \cite{DBLP:conf/icde/OlkenR92, olken1993random, garofalakis2001approximate, DBLP:conf/eurosys/AgarwalMPMMS13}.
For decision problems, exploratory analysis problems, and visualization, it often suffices to return an approximate query result bounded in confidence intervals.
For many common aggregates, a sample size of $k$ results in an approximation error $O(\frac{1}{\sqrt{k}})$, and therefore every additional $\epsilon$ factor of accuracy costs quadratically more. 
In applications where approximation can be tolerated, sampling avoids the expensive ``last mile" of processing and timely answers facilitate improved user experiences and faster analysis.

In traditional AQP, approximation necessarily sacrifices accuracy for reduced latency. 
However, the goal of SampleClean differs from AQP, as SampleClean trades off data cleaning cost for gradual improvements in query accuracy.
While SampleClean introduces approximation error, the data cleaning mitigates bias due to dirty data.
There is a break-even point where a sufficient amount of data is cleaned to facilitate an accurate approximation of queries on the cleaned data, and in this sense, sampling actually improves the accuracy of the query result.

SampleClean \cite{wang1999sample} and all of its extensions \cite{krishnan2015svc,krishnan2015acl,haas2015wisteria}, work in the \emph{budgeted data cleaning} setting. 
An analyst is allowed to apply an expensive data transformation $C(\cdot)$ to only $k\ll N$ rows in a relation.
One solution could be to draw $k$ records uniformly at random and apply data cleaning, e.g., a direct extension of AQP \cite{DBLP:conf/eurosys/AgarwalMPMMS13} to the cleaned sample.
However, data cleaning presents a number of statistical methodology problems that make this hard.
First, $C(\cdot)$ may change the sampling statistics, for example, duplicated records are more likely to be sampled.
Next, query processing on partially clean data, i.e., a mix of dirty and clean data, can lead to unreliable results due to the well known Simpsons Paradox.
Finally, high-dimensional analytics such as Machine Learning may be very sensitive to sample size, perhaps even more so than to dirty data, and techniques for avoiding sample size dependence are required.
Our research contribution in SampleClean is to study estimation techniques that avoid or mitigate these challenges.

There are two contrasting estimation techniques to address every budgeted data cleaning problem: \emph{direct} estimation and \emph{correction}.
The direct estimate applies a query, possibly with some re-weighting and scaling to account for data cleaning, to the cleaned sample of data. 
Alternatively, a sample of cleaned data can also be used to correct the error in a query result over the dirty data.
There is an interesting theoretical tradeoff between these approaches, where the direct approach is \emph{robust} as its accuracy is independent of the magnitude of data error, and the correction is \emph{sample-efficient} as its accuracy is less dependent on sample size than the direct estimate.

There are a number of new research opportunities at the intersection of data cleaning and approximate query processing.
We applied the same principles to other domains with expensive data transformations such as Materialized View Maintenance and Machine Learning.
In this article, we highlight three projects:

\vspace{0.5em}
\noindent \textbf{\sampleclean \cite{wang1999sample} \footnote{SampleClean refers to both the entire project and our initial publication.}: } \sampleclean estimates aggregate (\sumfunc, \countfunc, \avgfunc) queries using samples of clean data. \sampleclean reweights the data to compensate for changes in sampling statistics such that query result estimations remain unbiased and bounded in confidence intervals.

\vspace{0.5em}
\noindent \textbf{View Cleaning \cite{krishnan2015svc}: } View Cleaning generalizes the notion of data cleaning to include expensive incremental maintenance of out-of-date views. Staleness in materialized views (MVs) manifests itself as data error, i.e., a stale view has missing, superfluous, and incorrect rows.
Like data cleaning, eager MV maintenance is expensive.
Aggregate queries are approximated from a stale MV using a small sample of up-to-date data, resulting in bounded estimates.

\vspace{0.5em}
\noindent \textbf{ActiveClean \cite{krishnan2015acl}: } ActiveClean extends \sampleclean to a class of analytics problems called Convex Data Analytics; subsuming the aggregates studied in \sampleclean and including Machine Learning such as Support Vector Machines and Linear Regression. ActiveClean exploits the convex structure of the problem to prioritize cleaning data that is likely to affect the model. ActiveClean directly integrates cleaning into the model training loop and as a result gives a bounded approximation for a given cleaning budget.

\vspace{0.5em}

This article is organized as follows. Section 2 introduces the project and its main ideas. Section 4/5/6 describes \sampleclean, View Cleaning, and ActiveClean respectively. Section 7 reviews the related work in this field. In Section 8, we highlight some of the open problems and future directions of the SampleClean project. Finally, we conclude in Section 9.