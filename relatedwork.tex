\section{Related Work}


{\noindent \bf Approximate Query Processing:} AQP has been studied for more than two decades~\cite{garofalakis2001approximate,DBLP:journals/ftdb/CormodeGHJ12}.
Many AQP approaches~\cite{DBLP:journals/tods/ChaudhuriDN07,DBLP:conf/sigmod/AcharyaGPR99,DBLP:conf/cidr/SidirourgosKB11,DBLP:conf/sigmod/BabcockCD03,DBLP:conf/sigmod/HellersteinHW97,DBLP:journals/pvldb/PansareBJC11,DBLP:conf/sigmod/CondieCAHGTES10,DBLP:journals/pvldb/WuJOT09} were proposed, aiming to enable interactive query response times.
%The results in \cite{DBLP:journals/pvldb/NirkhiwaleDJ13} are also highly relevant to our work.
%Acharya et al.~\cite{DBLP:conf/sigmod/AcharyaGPR99} developed the Aqua system which is run on top of relational DBMS, and allows users to obtain approximate query answers in a fast response time. Chaudhuri et al.~\cite{DBLP:journals/tods/ChaudhuriDN07} studied how to create an optimal stratified random sample based on a given query workload, and proposed the STRAT algorithm that can make the workload queries achieve the best quality on the sample. Sidirourgos et al.~\cite{DBLP:conf/cidr/SidirourgosKB11} presented the SciBORQ architecture that creates biased multi-layered samples based on the scientific data exploration workload, and adaptively select a suitable sample to satisfy quality or time constraints during query execution. Agarwal et al.~\cite{DBLP:conf/eurosys/AgarwalMPMMS13} devised novel multi-dimensional stratified samples based on real-world big data analytics workloads, and built the BlinkDB system to utilize the created samples to support interactive query processing with error and response time constraints. Unlike the above approaches which create samples ahead of query time, online aggregation~\cite{DBLP:conf/sigmod/HellersteinHW97,DBLP:journals/pvldb/PansareBJC11,DBLP:conf/sigmod/CondieCAHGTES10,DBLP:journals/pvldb/WuJOT09} constructs samples in an online fashion, and gradually achieve more and more accurate results until users are satisfied about the current results. 
There are also many studies on creating other synopsis of the data, such as histograms or wavelets~\cite{DBLP:journals/ftdb/CormodeGHJ12}. While a substantial works on approximate query processing, these works mainly focus on how to deal with sampling errors, with little attention to data errors. 

\vspace{.5em}

{\noindent \bf Data Cleaning:} There have been many studies on various data-cleaning techniques, such as rule-based approaches~\cite{fan2012foundations,DBLP:conf/sigmod/DallachiesaEEEIOT13}, outlier detection~\cite{hellerstein2008quantitative,dasu2003exploratory}, filling missing values, and duplicate detection~\cite{conf/hdkm/Christen08, DBLP:conf/kdd/BilenkoM03, wang2012crowder}. In order to ensure reliable cleaning results, most of these techniques require human involvement.
For example, Fan et al.~\cite{DBLP:journals/pvldb/FanLMTY10} proposed to employ editing rules, master data and user confirmation to clean data, and proved that their approaches can always lead to correct cleaning results. Wang et al.~\cite{wang2012crowder} proposed a hybrid human-machine approach to detect duplicate entities in data, which can achieve higher detection accuracy than machine-only approaches. 
In SampleClean, the main focus is not on a specific data-cleaning technique, but rather on a new framework that enables a flexible trade-off between data cleaning cost and result quality.
Indeed, we can apply any data-cleaning technique to clean the sample data, and then utilize our framework to estimate query results based on the cleaned sample. 

\vspace{.5em}

{\noindent \bf Views and Cleaning:} Meliou et al. \cite{DBLP:conf/sigmod/MeliouGNS11} proposed a technique to trace errors in an MV to base data and find responsible erroneous tuples. 
They do not, however, propose a technique to correct the errors as in \svc.
Correcting general errors as in Meliou et al. is a hard constraint satisfaction problem.
However, in \svc, through our formalization of staleness, we have a model of how updates to the base data (modeled as errors) affect MVs, which allows us to both trace errors and clean them.
Wu and Madden \cite{DBLP:journals/pvldb/0002M13} did propose a model to correct ``outliers" in an MV through deletion of records in the base data.
This is a more restricted model of data cleaning than \svc, where the authors only consider changes to existing rows in an MV (no insertion or deletion) and do not handle the same generality of relational expressions (e.g., nested aggregates).
Challamalla et al. \cite{DBLP:conf/sigmod/ChalamallaIOP14} proposed an approximate technique for specifying errors as constraints on a materialized view and proposing changes to the base data such that these constraints can be satisfied.
While complementary, one major difference between the three works \cite{DBLP:conf/sigmod/MeliouGNS11, DBLP:journals/pvldb/0002M13, DBLP:conf/sigmod/ChalamallaIOP14} and \svc is that they require an explicit specification of erroneous rows in a materialized view.
Identifying whether a row is erroneous requires materialization and thus specifying the errors is equivalent to full incremental maintenance. 
However, while these approaches are not directly applicable for staleness, we see \svc as complementary to these works in the dirty data setting. 
